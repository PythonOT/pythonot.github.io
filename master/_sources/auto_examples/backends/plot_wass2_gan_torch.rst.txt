
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/backends/plot_wass2_gan_torch.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_backends_plot_wass2_gan_torch.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_backends_plot_wass2_gan_torch.py:


========================================
Wasserstein 2 Minibatch GAN with PyTorch
========================================

In this example we train a Wasserstein GAN using Wasserstein 2 on minibatches
as a distribution fitting term.

We want to train a generator :math:`G_\theta` that generates realistic
data from random noise drawn form a Gaussian :math:`\mu_n` distribution so
that the data is indistinguishable from true data in the data distribution
:math:`\mu_d`. To this end Wasserstein GAN [Arjovsky2017] aim at optimizing
the parameters :math:`\theta` of the generator with the following
optimization problem:

.. math::
     \min_{\theta} W(\mu_d,G_\theta\#\mu_n)


In practice we do not have access to the full distribution :math:`\mu_d` but
samples and we cannot compute the Wasserstein distance for lare dataset.
[Arjovsky2017] proposed to approximate the dual potential of Wasserstein 1
with a neural network recovering an optimization problem similar to GAN.
In this example
we will optimize the expectation of the Wasserstein distance over minibatches
at each iterations as proposed in [Genevay2018]. Optimizing the Minibatches
of the Wasserstein distance  has been studied in[Fatras2019].

[Arjovsky2017] Arjovsky, M., Chintala, S., & Bottou, L. (2017, July).
Wasserstein generative adversarial networks. In International conference
on machine learning (pp. 214-223). PMLR.

[Genevay2018] Genevay, Aude, Gabriel Peyr√©, and Marco Cuturi. "Learning generative models
with sinkhorn divergences." International Conference on Artificial Intelligence
and Statistics. PMLR, 2018.

[Fatras2019] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., & Courty, N.
(2020, June). Learning with minibatch Wasserstein: asymptotic and gradient
properties. In the 23nd International Conference on Artificial Intelligence
and Statistics (Vol. 108).

.. GENERATED FROM PYTHON SOURCE LINES 44-58

.. code-block:: default


    # Author: Remi Flamary <remi.flamary@polytechnique.edu>
    #
    # License: MIT License

    # sphinx_gallery_thumbnail_number = 3

    import numpy as np
    import matplotlib.pyplot as pl
    import torch
    from torch import nn
    import ot









.. GENERATED FROM PYTHON SOURCE LINES 59-61

Data generation
---------------

.. GENERATED FROM PYTHON SOURCE LINES 61-76

.. code-block:: default


    torch.manual_seed(1)
    sigma = 0.1
    n_dims = 2
    n_features = 2


    def get_data(n_samples):
        c = torch.rand(size=(n_samples, 1))
        angle = c * 2 * np.pi
        x = torch.cat((torch.cos(angle), torch.sin(angle)), 1)
        x += torch.randn(n_samples, 2) * sigma
        return x









.. GENERATED FROM PYTHON SOURCE LINES 77-79

Plot data
---------

.. GENERATED FROM PYTHON SOURCE LINES 79-88

.. code-block:: default


    # plot the distributions
    x = get_data(500)
    pl.figure(1)
    pl.scatter(x[:, 0], x[:, 1], label='Data samples from $\mu_d$', alpha=0.5)
    pl.title('Data distribution')
    pl.legend()





.. image-sg:: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_001.png
   :alt: Data distribution
   :srcset: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7fd7c8a83bd0>



.. GENERATED FROM PYTHON SOURCE LINES 89-91

Generator Model
---------------

.. GENERATED FROM PYTHON SOURCE LINES 91-109

.. code-block:: default


    # define the MLP model
    class Generator(torch.nn.Module):
        def __init__(self):
            super(Generator, self).__init__()
            self.fc1 = nn.Linear(n_features, 200)
            self.fc2 = nn.Linear(200, 500)
            self.fc3 = nn.Linear(500, n_dims)
            self.relu = torch.nn.ReLU()  # instead of Heaviside step fn

        def forward(self, x):
            output = self.fc1(x)
            output = self.relu(output)  # instead of Heaviside step fn
            output = self.fc2(output)
            output = self.relu(output)
            output = self.fc3(output)
            return output








.. GENERATED FROM PYTHON SOURCE LINES 110-112

Training the model
------------------

.. GENERATED FROM PYTHON SOURCE LINES 112-163

.. code-block:: default



    G = Generator()
    optimizer = torch.optim.RMSprop(G.parameters(), lr=0.001)

    # number of iteration and size of the batches
    n_iter = 500
    size_batch = 500

    # generate statis samples to see their trajectory along training
    n_visu = 100
    xnvisu = torch.randn(n_visu, n_features)
    xvisu = torch.zeros(n_iter, n_visu, n_dims)

    ab = torch.ones(size_batch) / size_batch
    losses = []


    for i in range(n_iter):

        # generate noise samples
        xn = torch.randn(size_batch, n_features)

        # generate data samples
        xd = get_data(size_batch)

        # generate sample along iterations
        xvisu[i, :, :] = G(xnvisu).detach()

        # generate smaples and compte distance matrix
        xg = G(xn)
        M = ot.dist(xg, xd)

        loss = ot.emd2(ab, ab, M)
        losses.append(float(loss.detach()))

        if i % 10 == 0:
            print("Iter: {:3d}, loss={}".format(i, losses[-1]))

        loss.backward()
        optimizer.step()

        del M

    pl.figure(2)
    pl.semilogy(losses)
    pl.grid()
    pl.title('Wasserstein distance')
    pl.xlabel("Iterations")





.. image-sg:: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_002.png
   :alt: Wasserstein distance
   :srcset: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Iter:   0, loss=0.9009847640991211
    Iter:  10, loss=0.19832603633403778
    Iter:  20, loss=0.1757493019104004
    Iter:  30, loss=0.2230413854122162
    Iter:  40, loss=0.05456963926553726
    Iter:  50, loss=0.10615786164999008
    Iter:  60, loss=0.07033250480890274
    Iter:  70, loss=0.10149531811475754
    Iter:  80, loss=0.039321497082710266
    Iter:  90, loss=0.04595823958516121
    Iter: 100, loss=0.05340520665049553
    Iter: 110, loss=0.03847748041152954
    Iter: 120, loss=0.05833236873149872
    Iter: 130, loss=0.03257983922958374
    Iter: 140, loss=0.034930042922496796
    Iter: 150, loss=0.03215852752327919
    Iter: 160, loss=0.027966026216745377
    Iter: 170, loss=0.030285801738500595
    Iter: 180, loss=0.038728710263967514
    Iter: 190, loss=0.10446169972419739
    Iter: 200, loss=0.02318563684821129
    Iter: 210, loss=0.032368823885917664
    Iter: 220, loss=0.032153062522411346
    Iter: 230, loss=0.022837283089756966
    Iter: 240, loss=0.041142676025629044
    Iter: 250, loss=0.0394926443696022
    Iter: 260, loss=0.016113894060254097
    Iter: 270, loss=0.03232649713754654
    Iter: 280, loss=0.02577916719019413
    Iter: 290, loss=0.043674662709236145
    Iter: 300, loss=0.042841263115406036
    Iter: 310, loss=0.02704923041164875
    Iter: 320, loss=0.026621874421834946
    Iter: 330, loss=0.034110747277736664
    Iter: 340, loss=0.021086430177092552
    Iter: 350, loss=0.04735469073057175
    Iter: 360, loss=0.015849558636546135
    Iter: 370, loss=0.023962466046214104
    Iter: 380, loss=0.023176442831754684
    Iter: 390, loss=0.02214888110756874
    Iter: 400, loss=0.020453479140996933
    Iter: 410, loss=0.03882370516657829
    Iter: 420, loss=0.023205207660794258
    Iter: 430, loss=0.01970205269753933
    Iter: 440, loss=0.018723461776971817
    Iter: 450, loss=0.02080429531633854
    Iter: 460, loss=0.031276822090148926
    Iter: 470, loss=0.0370972715318203
    Iter: 480, loss=0.04934333637356758
    Iter: 490, loss=0.07790705561637878

    Text(0.5, 23.52222222222222, 'Iterations')



.. GENERATED FROM PYTHON SOURCE LINES 164-166

Plot trajectories of generated samples along iterations
-------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 166-182

.. code-block:: default



    pl.figure(3, (10, 10))

    ivisu = [0, 10, 50, 100, 150, 200, 300, 400, 499]

    for i in range(9):
        pl.subplot(3, 3, i + 1)
        pl.scatter(xd[:, 0], xd[:, 1], label='Data samples from $\mu_d$', alpha=0.1)
        pl.scatter(xvisu[ivisu[i], :, 0], xvisu[ivisu[i], :, 1], label='Data samples from $G\#\mu_n$', alpha=0.5)
        pl.xticks(())
        pl.yticks(())
        pl.title('Iter. {}'.format(ivisu[i]))
        if i == 0:
            pl.legend()




.. image-sg:: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_003.png
   :alt: Iter. 0, Iter. 10, Iter. 50, Iter. 100, Iter. 150, Iter. 200, Iter. 300, Iter. 400, Iter. 499
   :srcset: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 183-185

Generate and visualize data
---------------------------

.. GENERATED FROM PYTHON SOURCE LINES 185-196

.. code-block:: default


    size_batch = 500
    xd = get_data(size_batch)
    xn = torch.randn(size_batch, 2)
    x = G(xn).detach().numpy()

    pl.figure(4)
    pl.scatter(xd[:, 0], xd[:, 1], label='Data samples from $\mu_d$', alpha=0.5)
    pl.scatter(x[:, 0], x[:, 1], label='Data samples from $G\#\mu_n$', alpha=0.5)
    pl.title('Sources and Target distributions')
    pl.legend()



.. image-sg:: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_004.png
   :alt: Sources and Target distributions
   :srcset: /auto_examples/backends/images/sphx_glr_plot_wass2_gan_torch_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7fd7c95ea890>




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 7 minutes  52.895 seconds)


.. _sphx_glr_download_auto_examples_backends_plot_wass2_gan_torch.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_wass2_gan_torch.py <plot_wass2_gan_torch.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_wass2_gan_torch.ipynb <plot_wass2_gan_torch.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
