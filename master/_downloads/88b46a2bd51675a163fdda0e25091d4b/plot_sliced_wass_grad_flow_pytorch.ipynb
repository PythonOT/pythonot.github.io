{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sliced Wasserstein barycenter and gradient flow with PyTorch\n\nIn this example we use the pytorch backend to optimize the sliced Wasserstein\nloss between two empirical distributions [31].\n\nIn the first example one we perform a\ngradient flow on the support of a distribution that minimize the sliced\nWasserstein distance as proposed in [36].\n\nIn the second example we optimize with a gradient descent the sliced\nWasserstein barycenter between two distributions as in [31].\n\n[31] Bonneel, Nicolas, et al. \"Sliced and radon wasserstein barycenters of\nmeasures.\" Journal of Mathematical Imaging and Vision 51.1 (2015): 22-45\n\n[36] Liutkus, A., Simsekli, U., Majewski, S., Durmus, A., & St\u00f6ter, F. R.\n(2019, May). Sliced-Wasserstein flows: Nonparametric generative modeling\nvia optimal transport and diffusions. In International Conference on\nMachine Learning (pp. 4104-4113). PMLR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: R\u00e9mi Flamary <remi.flamary@polytechnique.edu>\n#\n# License: MIT License\n\n# sphinx_gallery_thumbnail_number = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pylab as pl\nimport torch\nimport ot\nimport matplotlib.animation as animation\n\nI1 = pl.imread('../../data/redcross.png').astype(np.float64)[::5, ::5, 2]\nI2 = pl.imread('../../data/tooth.png').astype(np.float64)[::5, ::5, 2]\n\nsz = I2.shape[0]\nXX, YY = np.meshgrid(np.arange(sz), np.arange(sz))\n\nx1 = np.stack((XX[I1 == 0], YY[I1 == 0]), 1) * 1.0\nx2 = np.stack((XX[I2 == 0] + 60, -YY[I2 == 0] + 32), 1) * 1.0\nx3 = np.stack((XX[I2 == 0], -YY[I2 == 0] + 32), 1) * 1.0\n\npl.figure(1, (8, 4))\npl.scatter(x1[:, 0], x1[:, 1], alpha=0.5)\npl.scatter(x2[:, 0], x2[:, 1], alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sliced Wasserstein gradient flow with Pytorch\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# use pyTorch for our data\nx1_torch = torch.tensor(x1).to(device=device).requires_grad_(True)\nx2_torch = torch.tensor(x2).to(device=device)\n\n\nlr = 1e3\nnb_iter_max = 50\n\nx_all = np.zeros((nb_iter_max, x1.shape[0], 2))\n\nloss_iter = []\n\n# generator for random permutations\ngen = torch.Generator(device=device)\ngen.manual_seed(42)\n\nfor i in range(nb_iter_max):\n\n    loss = ot.sliced_wasserstein_distance(x1_torch, x2_torch, n_projections=20, seed=gen)\n\n    loss_iter.append(loss.clone().detach().cpu().numpy())\n    loss.backward()\n\n    # performs a step of projected gradient descent\n    with torch.no_grad():\n        grad = x1_torch.grad\n        x1_torch -= grad * lr / (1 + i / 5e1)  # step\n        x1_torch.grad.zero_()\n        x_all[i, :, :] = x1_torch.clone().detach().cpu().numpy()\n\nxb = x1_torch.clone().detach().cpu().numpy()\n\npl.figure(2, (8, 4))\npl.scatter(x1[:, 0], x1[:, 1], alpha=0.5, label='$\\mu^{(0)}$')\npl.scatter(x2[:, 0], x2[:, 1], alpha=0.5, label=r'$\\nu$')\npl.scatter(xb[:, 0], xb[:, 1], alpha=0.5, label='$\\mu^{(100)}$')\npl.title('Sliced Wasserstein gradient flow')\npl.legend()\nax = pl.axis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Animate trajectories of the gradient flow along iteration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure(3, (8, 4))\n\n\ndef _update_plot(i):\n    pl.clf()\n    pl.scatter(x1[:, 0], x1[:, 1], alpha=0.5, label='$\\mu^{(0)}$')\n    pl.scatter(x2[:, 0], x2[:, 1], alpha=0.5, label=r'$\\nu$')\n    pl.scatter(x_all[i, :, 0], x_all[i, :, 1], alpha=0.5, label='$\\mu^{(100)}$')\n    pl.title('Sliced Wasserstein gradient flow Iter. {}'.format(i))\n    pl.axis(ax)\n    return 1\n\n\nani = animation.FuncAnimation(pl.gcf(), _update_plot, nb_iter_max, interval=100, repeat_delay=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the Sliced Wasserstein Barycenter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x1_torch = torch.tensor(x1).to(device=device)\nx3_torch = torch.tensor(x3).to(device=device)\nxbinit = np.random.randn(500, 2) * 10 + 16\nxbary_torch = torch.tensor(xbinit).to(device=device).requires_grad_(True)\n\nlr = 1e3\nnb_iter_max = 50\n\nx_all = np.zeros((nb_iter_max, xbary_torch.shape[0], 2))\n\nloss_iter = []\n\n# generator for random permutations\ngen = torch.Generator(device=device)\ngen.manual_seed(42)\n\nalpha = 0.5\n\nfor i in range(nb_iter_max):\n\n    loss = alpha * ot.sliced_wasserstein_distance(xbary_torch, x3_torch, n_projections=50, seed=gen) \\\n        + (1 - alpha) * ot.sliced_wasserstein_distance(xbary_torch, x1_torch, n_projections=50, seed=gen)\n\n    loss_iter.append(loss.clone().detach().cpu().numpy())\n    loss.backward()\n\n    # performs a step of projected gradient descent\n    with torch.no_grad():\n        grad = xbary_torch.grad\n        xbary_torch -= grad * lr  # / (1 + i / 5e1)  # step\n        xbary_torch.grad.zero_()\n        x_all[i, :, :] = xbary_torch.clone().detach().cpu().numpy()\n\nxb = xbary_torch.clone().detach().cpu().numpy()\n\npl.figure(4, (8, 4))\npl.scatter(x1[:, 0], x1[:, 1], alpha=0.5, label='$\\mu$')\npl.scatter(x2[:, 0], x2[:, 1], alpha=0.5, label=r'$\\nu$')\npl.scatter(xb[:, 0] + 30, xb[:, 1], alpha=0.5, label='Barycenter')\npl.title('Sliced Wasserstein barycenter')\npl.legend()\nax = pl.axis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Animate trajectories of the barycenter along gradient descent\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pl.figure(5, (8, 4))\n\n\ndef _update_plot(i):\n    pl.clf()\n    pl.scatter(x1[:, 0], x1[:, 1], alpha=0.5, label='$\\mu^{(0)}$')\n    pl.scatter(x2[:, 0], x2[:, 1], alpha=0.5, label=r'$\\nu$')\n    pl.scatter(x_all[i, :, 0] + 30, x_all[i, :, 1], alpha=0.5, label='$\\mu^{(100)}$')\n    pl.title('Sliced Wasserstein barycenter Iter. {}'.format(i))\n    pl.axis(ax)\n    return 1\n\n\nani = animation.FuncAnimation(pl.gcf(), _update_plot, nb_iter_max, interval=100, repeat_delay=2000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}