{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learning sample marginal distribution with CO-Optimal Transport\n\nIn this example, we illustrate how to estimate the sample marginal distribution which minimizes\nthe CO-Optimal Transport distance [47]_ between two matrices. More precisely, given a source data\n$(X, \\mu_x^{(s)}, \\mu_x^{(f)})$ and a target matrix $Y$ associated with a fixed\nhistogram on features $\\mu_y^{(f)}$, we want to solve the following problem\n\n\\begin{align}\\min_{\\mu_y^{(s)} \\in \\Delta} \\text{COOT}\\left( (X, \\mu_x^{(s)}, \\mu_x^{(f)}), (Y, \\mu_y^{(s)}, \\mu_y^{(f)}) \\right)\\end{align}\n\nwhere $\\Delta$ is the probability simplex. This minimization is done with a\nsimple projected gradient descent in PyTorch. We use the automatic backend of POT that\nallows us to compute the CO-Optimal Transport distance with :func:`ot.coot.co_optimal_transport2`\nwith differentiable losses.\n\n.. [49] Redko, I., Vayer, T., Flamary, R., and Courty, N. (2020).\n   [CO-Optimal Transport](https://proceedings.neurips.cc/paper/2020/file/cc384c68ad503482fb24e6d1e3b512ae-Paper.pdf).\n   Advances in Neural Information Processing Systems, 33.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Remi Flamary <remi.flamary@unice.fr>\n#         Quang Huy Tran <quang-huy.tran@univ-ubs.fr>\n# License: MIT License\n\nfrom matplotlib.patches import ConnectionPatch\nimport torch\nimport numpy as np\n\nimport matplotlib.pyplot as pl\nimport ot\n\nfrom ot.coot import co_optimal_transport as coot\nfrom ot.coot import co_optimal_transport2 as coot2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate data\nThe source and clean target matrices are generated by\n$X_{i,j} = \\cos(\\frac{i}{n_1} \\pi) + \\cos(\\frac{j}{d_1} \\pi)$ and\n$Y_{i,j} = \\cos(\\frac{i}{n_2} \\pi) + \\cos(\\frac{j}{d_2} \\pi)$.\nThe target matrix is then contaminated by adding 5 row outliers.\nIntuitively, we expect that the estimated sample distribution should ignore these outliers,\ni.e. their weights should be zero.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(182)\n\nn1, d1 = 20, 16\nn2, d2 = 10, 8\nn = 15\n\nX = (\n    torch.cos(torch.arange(n1) * torch.pi / n1)[:, None] +\n    torch.cos(torch.arange(d1) * torch.pi / d1)[None, :]\n)\n\n# Generate clean target data mixed with outliers\nY_noisy = torch.randn((n, d2)) * 10.0\nY_noisy[:n2, :] = (\n    torch.cos(torch.arange(n2) * torch.pi / n2)[:, None] +\n    torch.cos(torch.arange(d2) * torch.pi / d2)[None, :]\n)\nY = Y_noisy[:n2, :]\n\nX, Y_noisy, Y = X.double(), Y_noisy.double(), Y.double()\n\nfig, axes = pl.subplots(nrows=1, ncols=3, figsize=(12, 5))\naxes[0].imshow(X, vmin=-2, vmax=2)\naxes[0].set_title('$X$')\n\naxes[1].imshow(Y, vmin=-2, vmax=2)\naxes[1].set_title('Clean $Y$')\n\naxes[2].imshow(Y_noisy, vmin=-2, vmax=2)\naxes[2].set_title('Noisy $Y$')\n\npl.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize the COOT distance with respect to the sample marginal distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "losses = []\nlr = 1e-3\nniter = 1000\n\nb = torch.tensor(ot.unif(n), requires_grad=True)\n\nfor i in range(niter):\n\n    loss = coot2(X, Y_noisy, wy_samp=b, log=False, verbose=False)\n    losses.append(float(loss))\n\n    loss.backward()\n\n    with torch.no_grad():\n        b -= lr * b.grad  # gradient step\n        b[:] = ot.utils.proj_simplex(b)  # projection on the simplex\n\n    b.grad.zero_()\n\n# Estimated sample marginal distribution and training loss curve\npl.plot(losses[10:])\npl.title('CO-Optimal Transport distance')\n\nprint(f\"Marginal distribution = {b.detach().numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the row and column alignments with the estimated sample marginal distribution\n\nClearly, the learned marginal distribution completely and successfully ignores the 5 outliers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, Y_noisy = X.numpy(), Y_noisy.numpy()\nb = b.detach().numpy()\n\npi_sample, pi_feature = coot(X, Y_noisy, wy_samp=b, log=False, verbose=True)\n\nfig = pl.figure(4, (9, 7))\npl.clf()\n\nax1 = pl.subplot(2, 2, 3)\npl.imshow(X, vmin=-2, vmax=2)\npl.xlabel('$X$')\n\nax2 = pl.subplot(2, 2, 2)\nax2.yaxis.tick_right()\npl.imshow(np.transpose(Y_noisy), vmin=-2, vmax=2)\npl.title(\"Transpose(Noisy $Y$)\")\nax2.xaxis.tick_top()\n\nfor i in range(n1):\n    j = np.argmax(pi_sample[i, :])\n    xyA = (d1 - .5, i)\n    xyB = (j, d2 - .5)\n    con = ConnectionPatch(xyA=xyA, xyB=xyB, coordsA=ax1.transData,\n                          coordsB=ax2.transData, color=\"black\")\n    fig.add_artist(con)\n\nfor i in range(d1):\n    j = np.argmax(pi_feature[i, :])\n    xyA = (i, -.5)\n    xyB = (-.5, j)\n    con = ConnectionPatch(\n        xyA=xyA, xyB=xyB, coordsA=ax1.transData, coordsB=ax2.transData, color=\"blue\")\n    fig.add_artist(con)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}